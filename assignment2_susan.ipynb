{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/afrahhassan/afrahhassan/blob/main/assignment2_susan.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1qSifWPkfTvv"
      },
      "source": [
        "\n",
        "\n",
        "---\n",
        "# <div align=\"center\"><font>  </font></div>\n",
        "# <div align=\"center\"><font> COSC 2793 Computational Machine Learning </font></div>\n",
        "## <div align=\"center\"> <font> Assignment 2: Machine Learning Project </font></div>\n",
        "## <div align=\"center\"> <font> S3930776 Afrah Hassan A Alshaikh Ali & S3569266 Chang Su </font></div>\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QQLtJkNQ_2Xs"
      },
      "source": [
        "- 1. Load Data into DataFrame (shape_df, type_df)\n",
        "- 2. check the dataset\n",
        "- 3. encode the labels\n",
        "- 4. rescale & data format\n",
        "- 5. split data\n",
        "- 6. flatten\n",
        "- 7. Basline models (X_train flattened, y_train_one-encoded)\n",
        "- 8. CNN model\n",
        "- 9. Data Augmentation\n",
        "- 10. Check the performance\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "twD8DWapfFnM"
      },
      "outputs": [],
      "source": [
        "#Import Libraries\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YZjcFQAm_5LR",
        "outputId": "515310e3-e6a1-43bc-aa53-d94a01175e70"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!cp -r \"/content/drive/MyDrive/Assignment2\" ."
      ],
      "metadata": {
        "id": "vVB627UR_67m"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9PmzuCjISE7S"
      },
      "source": [
        "# **1. Data Loading and Prepocessing**"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **1.1 Load the dataset**"
      ],
      "metadata": {
        "id": "VYwx86qmUEkt"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 278
        },
        "id": "73S8-Eyerd0H",
        "outputId": "2a6dbb13-34d3-4f34-a41c-75fd67bb3506"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Skipping non-image file: /content/drive/My Drive/Assignment2/trafficsigns_dataset/triangle/warning/.DS_Store\n",
            "Shape DataFrame:\n",
            "\n",
            "Type DataFrame:\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                          image_path  \\\n",
              "0  /content/drive/My Drive/Assignment2/trafficsig...   \n",
              "1  /content/drive/My Drive/Assignment2/trafficsig...   \n",
              "2  /content/drive/My Drive/Assignment2/trafficsig...   \n",
              "3  /content/drive/My Drive/Assignment2/trafficsig...   \n",
              "4  /content/drive/My Drive/Assignment2/trafficsig...   \n",
              "\n",
              "                                               image type_label  \n",
              "0  [[60, 64, 47, 39, 46, 33, 30, 33, 19, 24, 15, ...   crossing  \n",
              "1  [[73, 63, 59, 55, 46, 51, 52, 52, 61, 56, 57, ...   crossing  \n",
              "2  [[255, 255, 255, 255, 255, 254, 254, 254, 255,...   crossing  \n",
              "3  [[37, 31, 30, 31, 30, 42, 37, 36, 35, 37, 35, ...   crossing  \n",
              "4  [[117, 88, 77, 96, 87, 72, 79, 91, 80, 78, 76,...   crossing  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-c99bbea7-b7ec-4d8e-b6d9-ae692e332df0\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>image_path</th>\n",
              "      <th>image</th>\n",
              "      <th>type_label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>/content/drive/My Drive/Assignment2/trafficsig...</td>\n",
              "      <td>[[60, 64, 47, 39, 46, 33, 30, 33, 19, 24, 15, ...</td>\n",
              "      <td>crossing</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>/content/drive/My Drive/Assignment2/trafficsig...</td>\n",
              "      <td>[[73, 63, 59, 55, 46, 51, 52, 52, 61, 56, 57, ...</td>\n",
              "      <td>crossing</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>/content/drive/My Drive/Assignment2/trafficsig...</td>\n",
              "      <td>[[255, 255, 255, 255, 255, 254, 254, 254, 255,...</td>\n",
              "      <td>crossing</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>/content/drive/My Drive/Assignment2/trafficsig...</td>\n",
              "      <td>[[37, 31, 30, 31, 30, 42, 37, 36, 35, 37, 35, ...</td>\n",
              "      <td>crossing</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>/content/drive/My Drive/Assignment2/trafficsig...</td>\n",
              "      <td>[[117, 88, 77, 96, 87, 72, 79, 91, 80, 78, 76,...</td>\n",
              "      <td>crossing</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-c99bbea7-b7ec-4d8e-b6d9-ae692e332df0')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-c99bbea7-b7ec-4d8e-b6d9-ae692e332df0 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-c99bbea7-b7ec-4d8e-b6d9-ae692e332df0');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-7e529bd8-efa6-4bf0-8e35-2ba932bc7945\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-7e529bd8-efa6-4bf0-8e35-2ba932bc7945')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-7e529bd8-efa6-4bf0-8e35-2ba932bc7945 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "type_df",
              "summary": "{\n  \"name\": \"type_df\",\n  \"rows\": 3699,\n  \"fields\": [\n    {\n      \"column\": \"image_path\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 3699,\n        \"samples\": [\n          \"/content/drive/My Drive/Assignment2/trafficsigns_dataset/triangle/warning/00710_00002.png\",\n          \"/content/drive/My Drive/Assignment2/trafficsigns_dataset/round/noentry/01047_00002.png\",\n          \"/content/drive/My Drive/Assignment2/trafficsigns_dataset/square/parking/00035_00002.png\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"image\",\n      \"properties\": {\n        \"dtype\": \"object\",\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"type_label\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 16,\n        \"samples\": [\n          \"crossing\",\n          \"laneend\",\n          \"warning\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 5
        }
      ],
      "source": [
        "import os\n",
        "import cv2\n",
        "import pandas as pd\n",
        "\n",
        "def load_images_into_dataframes(data_dir):\n",
        "    shape_data = []\n",
        "    type_data = []\n",
        "\n",
        "    # Iterate through shape folders\n",
        "    for shape_folder in os.listdir(data_dir):\n",
        "        shape_folder_path = os.path.join(data_dir, shape_folder)\n",
        "\n",
        "        # Skip non-directory files\n",
        "        if not os.path.isdir(shape_folder_path):\n",
        "            continue\n",
        "\n",
        "        # Iterate through sign type folders\n",
        "        for sign_type_folder in os.listdir(shape_folder_path):\n",
        "            sign_type_folder_path = os.path.join(shape_folder_path, sign_type_folder)\n",
        "\n",
        "            # Skip non-directory files\n",
        "            if not os.path.isdir(sign_type_folder_path):\n",
        "                continue\n",
        "\n",
        "            # Iterate through image files\n",
        "            for image_file in os.listdir(sign_type_folder_path):\n",
        "                image_path = os.path.join(sign_type_folder_path, image_file)\n",
        "\n",
        "                # Check if the file is an image\n",
        "                if not image_path.endswith(('.jpg', '.jpeg', '.png', '.bmp', '.gif')):\n",
        "                    print(f\"Skipping non-image file: {image_path}\")\n",
        "                    continue\n",
        "\n",
        "                # Load the image\n",
        "                image = cv2.imread(image_path, cv2.IMREAD_GRAYSCALE)\n",
        "\n",
        "                # Check if the image is null\n",
        "                if image is None:\n",
        "                    print(f\"Skipping null image: {image_path}\")\n",
        "                    continue\n",
        "\n",
        "                # Append image and labels to the respective data lists\n",
        "                shape_data.append((image_path, image, shape_folder))\n",
        "                type_data.append((image_path, image, sign_type_folder))\n",
        "\n",
        "    # Create DataFrames from the collected data\n",
        "    shape_df = pd.DataFrame(shape_data, columns=['image_path', 'image', 'shape_label'])\n",
        "    type_df = pd.DataFrame(type_data, columns=['image_path', 'image', 'type_label'])\n",
        "\n",
        "    return shape_df, type_df\n",
        "\n",
        "# Example usage\n",
        "# data_dir = \"trafficsigns_dataset\"\n",
        "data_dir = \"/content/drive/My Drive/Assignment2/trafficsigns_dataset\"\n",
        "shape_df, type_df = load_images_into_dataframes(data_dir)\n",
        "\n",
        "print(\"Shape DataFrame:\")\n",
        "shape_df.head()\n",
        "\n",
        "print(\"\\nType DataFrame:\")\n",
        "type_df.head()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0fQk_erpDklD"
      },
      "source": [
        "This function load_data_into_dataframe loads image data from a directory into a DataFrame. It iterates through the directory structure to find image files and assigns them labels based on the folder structure. Then, it creates a DataFrame with columns for image paths, shape labels, and sign labels. Finally, it returns the DataFrame containing the loaded data."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **1.2 EDA**"
      ],
      "metadata": {
        "id": "BMQuvafx74Te"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "an3EywS-Oouj",
        "outputId": "64597f4c-182b-45c9-f702-fdfd9e70e2a1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "DataFrame Size:\n",
            "(3699, 3)\n",
            "\n",
            "DataFrame Information:\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 3699 entries, 0 to 3698\n",
            "Data columns (total 3 columns):\n",
            " #   Column       Non-Null Count  Dtype \n",
            "---  ------       --------------  ----- \n",
            " 0   image_path   3699 non-null   object\n",
            " 1   image        3699 non-null   object\n",
            " 2   shape_label  3699 non-null   object\n",
            "dtypes: object(3)\n",
            "memory usage: 86.8+ KB\n",
            "None\n",
            "\n",
            "DataFrame Descriptive Statistics:\n"
          ]
        }
      ],
      "source": [
        "# Check the size of the Shape DataFrame\n",
        "print(\"DataFrame Size:\")\n",
        "print(shape_df.shape)\n",
        "\n",
        "# Display basic information about the Shape DataFrame\n",
        "print(\"\\nDataFrame Information:\")\n",
        "print(shape_df.info())\n",
        "\n",
        "# Display descriptive statistics of the Shape DataFrame\n",
        "print(\"\\nDataFrame Descriptive Statistics:\")\n",
        "print(shape_df.describe())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bOwZORo2_2Xv"
      },
      "outputs": [],
      "source": [
        "# Check the size of the Type DataFrame\n",
        "print(\"DataFrame Size:\")\n",
        "print(type_df.shape)\n",
        "\n",
        "# Display basic information about the Type DataFrame\n",
        "print(\"\\nDataFrame Information:\")\n",
        "print(type_df.info())\n",
        "\n",
        "# Display descriptive statistics of the Type DataFrame\n",
        "print(\"\\nDataFrame Descriptive Statistics:\")\n",
        "print(type_df.describe())"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "type_df['image']"
      ],
      "metadata": {
        "id": "pjiM7UYQ16a9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Gn5tGp-BPM7V"
      },
      "outputs": [],
      "source": [
        "# Number of data points\n",
        "num_data_points_1 = len(type_df)\n",
        "print(\"Number of Data Points:\", num_data_points_1)\n",
        "\n",
        "num_data_points_2 = len(shape_df)\n",
        "print(\"Number of Data Points:\", num_data_points_2)\n",
        "\n",
        "# Count of unique shapes\n",
        "num_unique_shapes = shape_df['shape_label'].nunique()\n",
        "print(\"Number of Unique Image Shapes:\", num_unique_shapes)\n",
        "\n",
        "# Count of unique types\n",
        "num_unique_types = type_df['type_label'].nunique()\n",
        "print(\"Number of Unique Image Types:\", num_unique_types)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sUySSMJyPvvp"
      },
      "outputs": [],
      "source": [
        "# Number of each shape\n",
        "shape_counts = shape_df['shape_label'].value_counts()\n",
        "print(\"Number of Each Shape Type:\")\n",
        "print(shape_counts)\n",
        "\n",
        "# Number of each type\n",
        "type_counts = type_df['type_label'].value_counts()\n",
        "print(\"\\nNumber of Each Type:\")\n",
        "print(type_counts)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TtNMXjpT_2Xv"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "def plot_samples(data_df, num_samples=5):\n",
        "    fig, axes = plt.subplots(1, num_samples, figsize=(15, 3))\n",
        "\n",
        "    for i in range(num_samples):\n",
        "        sample = data_df.sample()  # Sampling from the DataFrame passed as input\n",
        "        image = sample['image'].values[0]  # Assuming 'image' contains the image data\n",
        "        label = sample['shape_label'].values[0] if 'shape_label' in data_df.columns else sample['type_label'].values[0]\n",
        "        axes[i].imshow(image, cmap='gray')  # Assuming grayscale images\n",
        "        axes[i].set_title(label)\n",
        "        axes[i].axis('off')\n",
        "\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "# Plot sample images from the shape DataFrame\n",
        "print(\"Samples from Shape DataFrame:\")\n",
        "plot_samples(shape_df)\n",
        "\n",
        "# Plot sample images from the type DataFrame\n",
        "print(\"Samples from Type DataFrame:\")\n",
        "plot_samples(type_df)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rSKYdSRO_2Xw"
      },
      "source": [
        "Convert the shape labels into numerical labels: This step creates the target vector y_shape containing numerical labels for each shape category.\n",
        "Perform one-hot encoding on the shape labels:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pTsr7ero_2Xw"
      },
      "source": [
        "## **1.3 Preprocess Data - Encoding**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9fDL-XrJ_2Xw"
      },
      "outputs": [],
      "source": [
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "# Create LabelEncoder instance\n",
        "label_encoder = LabelEncoder()\n",
        "\n",
        "# Fit and transform the shape labels\n",
        "shape_labels_encoded = label_encoder.fit_transform(shape_df['shape_label'])\n",
        "\n",
        "# Add the encoded labels as a new column in shape_df\n",
        "shape_df['encoded_shape_label'] = shape_labels_encoded\n",
        "\n",
        "# Display the updated shape_df\n",
        "shape_df.head()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove duplicates based on 'image_path' column\n",
        "shape_df_unique = shape_df.drop_duplicates(subset='image')\n",
        "\n",
        "# Print the shape before and after removing duplicates\n",
        "print(\"Shape of shape_df before removing duplicates:\", shape_df.shape)\n",
        "print(\"Shape of shape_df after removing duplicates:\", shape_df_unique.shape)\n"
      ],
      "metadata": {
        "id": "b8Vm2d7xLP9g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Assuming 'type_df' is your DataFrame containing type data\n",
        "\n",
        "# Remove duplicates based on 'image_path' column\n",
        "type_df_unique = type_df.drop_duplicates(subset='image')\n",
        "\n",
        "# Print the shape before and after removing duplicates\n",
        "print(\"Shape of type_df before removing duplicates:\", type_df.shape)\n",
        "print(\"Shape of type_df after removing duplicates:\", type_df_unique.shape)\n"
      ],
      "metadata": {
        "id": "LJKfaMa7Lg1j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A0uIzn6K_2Xw"
      },
      "outputs": [],
      "source": [
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "# Create LabelEncoder instance\n",
        "label_encoder = LabelEncoder()\n",
        "\n",
        "# Fit and transform the type labels\n",
        "type_labels_encoded = label_encoder.fit_transform(type_df['type_label'])\n",
        "\n",
        "# Add the encoded labels as a new column in type_df\n",
        "type_df['encoded_type_label'] = type_labels_encoded\n",
        "\n",
        "# Display the updated type_df\n",
        "type_df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sTcc1RvP_2Xw"
      },
      "outputs": [],
      "source": [
        "# Count the unique number of encoded shape labels\n",
        "num_unique_encoded_labels = shape_df['encoded_shape_label'].nunique()\n",
        "\n",
        "# Print the number of unique encoded shape labels\n",
        "print(\"Number of Unique Encoded Shape Labels:\", num_unique_encoded_labels)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y9B9MjnM_2Xw"
      },
      "outputs": [],
      "source": [
        "# Count the unique number of encoded type labels\n",
        "num_unique_encoded_labels = type_df['encoded_type_label'].nunique()\n",
        "\n",
        "# Print the number of unique encoded type labels\n",
        "print(\"Number of Unique Encoded Type Labels:\", num_unique_encoded_labels)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fn_6dnKV_2Xx"
      },
      "outputs": [],
      "source": [
        "# Function to check the shape of images\n",
        "def check_image_shape(df):\n",
        "    for image in df['image']:\n",
        "        print(\"Image shape:\", image.shape)\n",
        "\n",
        "# Check the shape of images in shape_df\n",
        "check_image_shape(shape_df)\n",
        "\n",
        "# Check the shape of images in shape_df\n",
        "check_image_shape(type_df)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "niWflxP-2D8Y"
      },
      "source": [
        "## **1.4 Split the Data into train, test and val sets**\n",
        "\n",
        "- Rescaling and Formatting the Images\n",
        "\n",
        "- Split the Data: Stratified sampling is a sampling method where the population is divided into homogeneous subgroups called strata, and then samples are randomly selected from each stratum in proportion to the population size of the stratum. This ensures that the sample represents the population's diversity more accurately."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# from sklearn.model_selection import train_test_split\n",
        "# import numpy as np\n",
        "\n",
        "# # Split data into train and test sets for shape data\n",
        "# X_shape_train, X_shape_test, y_shape_train, y_shape_test = train_test_split(\n",
        "#     shape_df['image_path'], shape_df['encoded_shape_label'], test_size=0.2, stratify=shape_df['shape_label'])\n",
        "\n",
        "# # Split train set into train and validation sets for shape data\n",
        "# X_shape_train, X_shape_val, y_shape_train, y_shape_val = train_test_split(\n",
        "#     X_shape_train, y_shape_train, test_size=0.25, stratify=y_shape_train)\n",
        "\n",
        "# # Split data into train and test sets for type data\n",
        "# X_type_train, X_type_test, y_type_train, y_type_test = train_test_split(\n",
        "#     type_df['image_path'], type_df['encoded_type_label'], test_size=0.2, stratify=type_df['type_label'])\n",
        "\n",
        "# # Split train set into train and validation sets for type data\n",
        "# X_type_train, X_type_val, y_type_train, y_type_val = train_test_split(\n",
        "#     X_type_train, y_type_train, test_size=0.25, stratify=y_type_train)\n",
        "\n",
        "# # Convert to numpy arrays\n",
        "# X_shape_train = np.array(X_shape_train)\n",
        "# X_shape_val = np.array(X_shape_val)\n",
        "# X_shape_test = np.array(X_shape_test)\n",
        "# y_shape_train = np.array(y_shape_train)\n",
        "# y_shape_val = np.array(y_shape_val)\n",
        "# y_shape_test = np.array(y_shape_test)\n",
        "\n",
        "# X_type_train = np.array(X_type_train)\n",
        "# X_type_val = np.array(X_type_val)\n",
        "# X_type_test = np.array(X_type_test)\n",
        "# y_type_train = np.array(y_type_train)\n",
        "# y_type_val = np.array(y_type_val)\n",
        "# y_type_test = np.array(y_type_test)"
      ],
      "metadata": {
        "id": "QxI_tDISrJFy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # Print shapes of the data sets after splitting for shape data\n",
        "# print(\"\\nShape of X_shape_train:\", X_shape_train.shape)\n",
        "# print(\"Shape of X_shape_val:\", X_shape_val.shape)\n",
        "# print(\"Shape of X_shape_test:\", X_shape_test.shape)\n",
        "# print(\"Shape of y_shape_train:\", y_shape_train.shape)\n",
        "# print(\"Shape of y_shape_val:\", y_shape_val.shape)\n",
        "# print(\"Shape of y_shape_test:\", y_shape_test.shape)"
      ],
      "metadata": {
        "id": "wfT7IM-PU9ud"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # Print shapes of the data sets after splitting for type data\n",
        "# print(\"\\nShape of X_type_train:\", X_type_train.shape)\n",
        "# print(\"Shape of X_type_val:\", X_type_val.shape)\n",
        "# print(\"Shape of X_type_test:\", X_type_test.shape)\n",
        "# print(\"Shape of y_type_train:\", y_type_train.shape)\n",
        "# print(\"Shape of y_type_val:\", y_type_val.shape)\n",
        "# print(\"Shape of y_type_test:\", y_type_test.shape)"
      ],
      "metadata": {
        "id": "GiWui9cxU_X_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Split data into train and test sets for shape data\n",
        "X_shape_train, X_shape_test, y_shape_train, y_shape_test = train_test_split(\n",
        "    shape_df['image_path'], shape_df['encoded_shape_label'], test_size=0.2, stratify=shape_df['encoded_shape_label'])\n",
        "\n",
        "# Split train set into train and validation sets for shape data\n",
        "X_shape_train, X_shape_val, y_shape_train, y_shape_val = train_test_split(\n",
        "    X_shape_train, y_shape_train, test_size=0.25, stratify=y_shape_train)\n",
        "\n",
        "# Split data into train and test sets for type data\n",
        "X_type_train, X_type_test, y_type_train, y_type_test = train_test_split(\n",
        "    type_df['image_path'], type_df['encoded_type_label'], test_size=0.2, stratify=type_df['encoded_type_label'])\n",
        "\n",
        "# Split train set into train and validation sets for type data\n",
        "X_type_train, X_type_val, y_type_train, y_type_val = train_test_split(\n",
        "    X_type_train, y_type_train, test_size=0.25, stratify=y_type_train)"
      ],
      "metadata": {
        "id": "ZvS3MGEiV1ma"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ftm0W7CDDpQ2"
      },
      "source": [
        "This code will split the data into training and testing sets for each sign label, ensuring a balanced distribution of data across the splits."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Shape of Shape Data:\")\n",
        "print(\"X_shape_train shape:\", X_shape_train.shape)\n",
        "print(\"X_shape_val shape:\", X_shape_val.shape)\n",
        "print(\"X_shape_test shape:\", X_shape_test.shape)\n",
        "print(\"y_shape_train shape:\", y_shape_train.shape)\n",
        "print(\"y_shape_val shape:\", y_shape_val.shape)\n",
        "print(\"y_shape_test shape:\", y_shape_test.shape)"
      ],
      "metadata": {
        "id": "FT9ktJdWV1pA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "For Shape Data:\n",
        "\n",
        "- X_shape_train: Training set features for shape classification\n",
        "- X_shape_val: Validation set features for shape classification\n",
        "- X_shape_test: Test set features for shape classification\n",
        "- y_shape_train: Training set labels for shape classification\n",
        "- y_shape_val: Validation set labels for shape classification\n",
        "- y_shape_test: Test set labels for shape classification -"
      ],
      "metadata": {
        "id": "7Z_hcHsMZXCn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\nShape of Type Data:\")\n",
        "print(\"X_type_train shape:\", X_type_train.shape)\n",
        "print(\"X_type_val shape:\", X_type_val.shape)\n",
        "print(\"X_type_test shape:\", X_type_test.shape)\n",
        "print(\"y_type_train shape:\", y_type_train.shape)\n",
        "print(\"y_type_val shape:\", y_type_val.shape)\n",
        "print(\"y_type_test shape:\", y_type_test.shape)"
      ],
      "metadata": {
        "id": "NtlO3X0kWmoF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "For Type Data:\n",
        "\n",
        "- X_type_train: Training set features for type classification\n",
        "- X_type_val: Validation set features for type classification\n",
        "- X_type_test: Test set features for type classification\n",
        "- y_type_train: Training set labels for type classification\n",
        "- y_type_val: Validation set labels for type classification\n",
        "- y_type_test: Test set labels for type classification"
      ],
      "metadata": {
        "id": "HC00rlOUZgNf"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3PTfu6wG_2Xx"
      },
      "source": [
        "## **1.5 Flatten the images**"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Flattening the images is a necessary step when using a Multi-Layer Perceptron (MLP) model because MLPs require input data to be in the form of one-dimensional arrays or vectors.\n",
        "\n",
        "Each image in the dataset is represented as a two-dimensional array (e.g., 28x28 pixels for grayscale images). When we feed these images into an MLP, we need to flatten them into one-dimensional arrays so that each pixel value becomes a separate input feature."
      ],
      "metadata": {
        "id": "M7KXzQ19aWuA"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TUF9RpVY_2Xx"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "# from keras.preprocessing.image import ImageDataGenerator\n",
        "from PIL import Image\n",
        "\n",
        "# Function to flatten images\n",
        "def flatten_images(X):\n",
        "    flattened_images = []\n",
        "    for image_path in X:\n",
        "        image = Image.open(image_path)\n",
        "        image = np.array(image)\n",
        "        flattened_image = image.flatten()\n",
        "        flattened_images.append(flattened_image)\n",
        "    return np.array(flattened_images)\n",
        "\n",
        "# Flatten the images for shape data in training and testing sets\n",
        "X_shape_train_flat = flatten_images(X_shape_train)\n",
        "X_shape_test_flat = flatten_images(X_shape_test)\n",
        "X_shape_val_flat = flatten_images(X_shape_val)\n",
        "\n",
        "# Flatten the images for type data in training and testing sets\n",
        "X_type_train_flat = flatten_images(X_type_train)\n",
        "X_type_test_flat = flatten_images(X_type_test)\n",
        "X_type_val_flat = flatten_images(X_type_val)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Print shapes of the flattened data sets\n",
        "print(\"\\nShape of X_shape_train_flat:\", X_shape_train_flat.shape)\n",
        "print(\"Shape of X_shape_test_flat:\", X_shape_test_flat.shape)\n",
        "print(\"Shape of X_shape_val_flat:\", X_shape_val_flat.shape)"
      ],
      "metadata": {
        "id": "yGFL21kZaQSC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\nShape of X_type_train_flat:\", X_type_train_flat.shape)\n",
        "print(\"Shape of X_type_test_flat:\", X_type_test_flat.shape)\n",
        "print(\"Shape of X_type_val_flat:\", X_type_val_flat.shape)"
      ],
      "metadata": {
        "id": "VI7VBEkAaQaH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SJ7ZnJvM_2Xx"
      },
      "source": [
        "# **2. Shape Classification**\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **2.1 Baseline Model (MLP Model with Two Layers Hidden)**"
      ],
      "metadata": {
        "id": "CRzqflY6bFhD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def plot_learning_curve(train_loss, val_loss, train_metric, val_metric, metric_name='Accuracy'):\n",
        "    plt.figure(figsize=(10,5))\n",
        "\n",
        "    plt.subplot(1,2,1)\n",
        "    plt.plot(train_loss, 'r--', label='Train')\n",
        "    plt.plot(val_loss, 'b--', label='Validation')\n",
        "    plt.xlabel(\"Epochs\")\n",
        "    plt.ylabel(\"Loss\")\n",
        "    plt.legend(loc='upper left')\n",
        "\n",
        "    plt.subplot(1,2,2)\n",
        "    plt.plot(train_metric, 'r--', label='Train')\n",
        "    plt.plot(val_metric, 'b--', label='Validation')\n",
        "    plt.xlabel(\"Epochs\")\n",
        "    plt.ylabel(metric_name)\n",
        "    plt.legend(loc='upper left')\n",
        "\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "f-AGCSV8foGj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_shape_train_flat.shape"
      ],
      "metadata": {
        "id": "Pqx29Iy4HNE4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_shape_train.shape"
      ],
      "metadata": {
        "id": "XniwNlT2HMrA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This baseline model provides a starting point for our investigation into classifying traffic sign shapes using machine learning algorithms.\n"
      ],
      "metadata": {
        "id": "2JkMDoJUegtd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "\n",
        "# Define the MLP model\n",
        "def build_mlp(input_shape, num_classes):\n",
        "    model = Sequential([\n",
        "        Dense(128, activation='sigmoid', input_shape=(input_shape,)),\n",
        "        Dense(64, activation='sigmoid'),\n",
        "        Dense(num_classes, activation='softmax')\n",
        "    ])\n",
        "    return model\n",
        "\n",
        "# Set input shape\n",
        "input_shape = X_shape_train_flat.shape[1]\n",
        "\n",
        "# Number of classes\n",
        "num_classes = len(np.unique(y_shape_train))\n",
        "\n",
        "# Build the MLP model\n",
        "model_mpl_shape = build_mlp(input_shape, num_classes)\n",
        "\n",
        "# Compile the model\n",
        "model_mpl_shape.compile(loss='sparse_categorical_crossentropy',\n",
        "                   optimizer='SGD',\n",
        "                   metrics=['accuracy'])\n",
        "\n",
        "# Print model summary\n",
        "model_mpl_shape.summary()\n"
      ],
      "metadata": {
        "id": "1resyYmSt6r6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train the model with validation data\n",
        "history_mlp_shape = model_mpl_shape.fit(X_shape_train_flat, y_shape_train, epochs=10, batch_size=32, validation_data=(X_shape_val_flat, y_shape_val))"
      ],
      "metadata": {
        "id": "h8UmLPI3cZTl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Check if history object contains the correct keys\n",
        "print(history_mlp_shape.history.keys())\n",
        "\n",
        "# Evaluate the model on the train, validation, and test data\n",
        "train_loss_mlp_shape, train_accuracy_mlp_shape = model_mpl_shape.evaluate(X_shape_train_flat, y_shape_train, verbose=1)\n",
        "val_loss_mlp_shape, val_accuracy_mlp_shape = model_mpl_shape.evaluate(X_shape_val_flat, y_shape_val, verbose=1)\n",
        "test_loss_mlp_shape, test_accuracy_mlp_shape = model_mpl_shape.evaluate(X_shape_test_flat, y_shape_test, verbose=1)\n",
        "\n",
        "# Print the results\n",
        "print(\"Train Accuracy (MLP) for Shapes:\", train_accuracy_mlp_shape)\n",
        "print(\"Train Loss (MLP)for Shapes:\", train_loss_mlp_shape)\n",
        "print(\"Validation Accuracy (MLP) for Shapes:\", val_accuracy_mlp_shape)\n",
        "print(\"Validation Loss (MLP) for Shapes:\", val_loss_mlp_shape)\n",
        "print(\"Test Accuracy (MLP) for Shapes:\", test_accuracy_mlp_shape)\n",
        "print(\"Test Loss (MLP) for Shapes:\", test_loss_mlp_shape)\n"
      ],
      "metadata": {
        "id": "DwKewqdYcbx2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot learning curves using the custom function\n",
        "plot_learning_curve(history_cnn_shape.history['loss'], history_cnn_shape.history['val_loss'],\n",
        "                    history_cnn_shape.history['accuracy'], history_cnn_shape.history['val_accuracy'],\n",
        "                    metric_name='Accuracy')\n"
      ],
      "metadata": {
        "id": "a9tTrEg-gILZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The first plot shows the training and validation loss for the MLP model over the epochs. The training loss decreases steadily over time, while the validation loss initially decreases and then starts to increase after a certain number of epochs. This indicates that the model is overfitting to the training data.\n",
        "\n",
        "The second plot shows the training and validation accuracy for the MLP model over the epochs. The training accuracy increases steadily over time, while the validation accuracy initially increases and then starts to plateau after a certain number of epochs. This also indicates that the model is overfitting to the training data.\n",
        "\n",
        "Overall, the plots show that the MLP model is not performing well on the validation and test data. This is likely due to overfitting. To improve the performance of the model, we can try using regularization techniques or reducing the number of epochs.\n"
      ],
      "metadata": {
        "id": "XmRXzMAD5iFH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "# Predict classes for test data\n",
        "y_pred_1 = np.argmax(model_mpl_shape.predict(X_shape_test_flat), axis=1)\n",
        "\n",
        "# Generate classification report\n",
        "report = classification_report(y_shape_test, y_pred_1)\n",
        "\n",
        "# Print the classification report\n",
        "print(\"Classification Report:\\n\", report)"
      ],
      "metadata": {
        "id": "KAz7JuMOCo99"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "MLP model for shape classification is achieving an accuracy around 47.6% on the training, validation, and test sets. However, the precision, recall, and F1-score for each class are quite low. Here's a breakdown of the results:\n",
        "\n",
        "- Train Accuracy (MLP) for Shapes: 47.59%\n",
        "- Train Loss (MLP) for Shapes: 1.2547\n",
        "- Validation Accuracy (MLP) for Shapes: 47.57%\n",
        "- Validation Loss (MLP) for Shapes: 1.2599\n",
        "- Test Accuracy (MLP) for Shapes: 47.57%\n",
        "- Test Loss (MLP) for Shapes: 1.2651\n",
        "\n"
      ],
      "metadata": {
        "id": "g1J1fFRoeHSP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Classification Report:**\n",
        "\n",
        "- The precision, recall, and F1-score for each class indicate poor performance.\n",
        "\n",
        "- The overall accuracy is around 48%, but the model struggles to correctly classify instances across different classes.\n",
        "\n",
        "Given these results, it's apparent that the MLP model is not performing well for shape classification. Further analysis and potentially model adjustments are needed to improve its performance."
      ],
      "metadata": {
        "id": "pIOkA40PeNto"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import confusion_matrix\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Compute confusion matrix for test data\n",
        "conf_mat_test_mlp = confusion_matrix(y_shape_test, y_pred_1)\n",
        "\n",
        "# Plot confusion matrix for test data\n",
        "plt.figure(figsize=(10, 8))\n",
        "sns.heatmap(conf_mat_test_mlp, annot=True, fmt='d', cmap='Blues', cbar=False)\n",
        "plt.title('Confusion Matrix - Test Data')\n",
        "plt.xlabel('Predicted Label')\n",
        "plt.ylabel('True Label')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "D7hJArhXvY3b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "The confusion matrix for the test data reveals that the MLP model is struggling to correctly classify instances across different classes."
      ],
      "metadata": {
        "id": "FtX7Dk3-fUhF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **2.2 Optimized Model: Convolutional Neural Network (CNN)**"
      ],
      "metadata": {
        "id": "98hL0CVPHjh9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, we've chosen the second model, CNN, because it's particularly effective for image classification tasks like the one at hand. Convolutional Neural Networks (CNNs) are adept at learning hierarchical representations of images, capturing features at different levels of abstraction. By using convolutional and pooling layers, CNNs can efficiently extract spatial patterns from input images, making them well-suited for tasks where the spatial relationships between pixels are crucial for classification."
      ],
      "metadata": {
        "id": "B5BkhLLN62yi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **2.1 Baseline CNN Model**"
      ],
      "metadata": {
        "id": "8DvN0KJ3IDMg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "\n",
        "# Initialize lists to store problematic image paths\n",
        "problematic_paths = []\n",
        "\n",
        "# Concatenate shape training, validation, and testing data\n",
        "X_train_shape_cnn = []\n",
        "for path in X_shape_train:\n",
        "    image = cv2.imread(str(path), cv2.IMREAD_GRAYSCALE)\n",
        "    if image is None:\n",
        "        problematic_paths.append(path)\n",
        "    else:\n",
        "        X_train_shape_cnn.append(image.reshape(28, 28, 1))\n",
        "\n",
        "X_train_shape_cnn = np.array(X_train_shape_cnn)\n",
        "\n",
        "X_val_shape_cnn = []\n",
        "for path in X_shape_val:\n",
        "    image = cv2.imread(str(path), cv2.IMREAD_GRAYSCALE)\n",
        "    if image is None:\n",
        "        problematic_paths.append(path)\n",
        "    else:\n",
        "        X_val_shape_cnn.append(image.reshape(28, 28, 1))\n",
        "\n",
        "X_val_shape_cnn = np.array(X_val_shape_cnn)\n",
        "\n",
        "X_test_shape_cnn = []\n",
        "for path in X_shape_test:\n",
        "    image = cv2.imread(str(path), cv2.IMREAD_GRAYSCALE)\n",
        "    if image is None:\n",
        "        problematic_paths.append(path)\n",
        "    else:\n",
        "        X_test_shape_cnn.append(image.reshape(28, 28, 1))\n",
        "\n",
        "X_test_shape_cnn = np.array(X_test_shape_cnn)\n",
        "\n",
        "# Concatenate type training, validation, and testing data\n",
        "X_train_type_cnn = []\n",
        "for path in X_type_train:\n",
        "    image = cv2.imread(str(path), cv2.IMREAD_GRAYSCALE)\n",
        "    if image is None:\n",
        "        problematic_paths.append(path)\n",
        "    else:\n",
        "        X_train_type_cnn.append(image.reshape(28, 28, 1))\n",
        "\n",
        "X_train_type_cnn = np.array(X_train_type_cnn)\n",
        "\n",
        "X_val_type_cnn = []\n",
        "for path in X_type_val:\n",
        "    image = cv2.imread(str(path), cv2.IMREAD_GRAYSCALE)\n",
        "    if image is None:\n",
        "        problematic_paths.append(path)\n",
        "    else:\n",
        "        X_val_type_cnn.append(image.reshape(28, 28, 1))\n",
        "\n",
        "X_val_type_cnn = np.array(X_val_type_cnn)\n",
        "\n",
        "X_test_type_cnn = []\n",
        "for path in X_type_test:\n",
        "    image = cv2.imread(str(path), cv2.IMREAD_GRAYSCALE)\n",
        "    if image is None:\n",
        "        problematic_paths.append(path)\n",
        "    else:\n",
        "        X_test_type_cnn.append(image.reshape(28, 28, 1))\n",
        "\n",
        "X_test_type_cnn = np.array(X_test_type_cnn)\n",
        "\n",
        "# Print the shapes of the concatenated data\n",
        "print(\"Shape of X_train_shape_cnn:\", X_train_shape_cnn.shape)\n",
        "print(\"Shape of X_val_shape_cnn:\", X_val_shape_cnn.shape)\n",
        "print(\"Shape of X_test_shape_cnn:\", X_test_shape_cnn.shape)\n",
        "\n",
        "print(\"Shape of X_train_type_cnn:\", X_train_type_cnn.shape)\n",
        "print(\"Shape of X_val_type_cnn:\", X_val_type_cnn.shape)\n",
        "print(\"Shape of X_test_type_cnn:\", X_test_type_cnn.shape)\n",
        "\n",
        "# Print problematic paths\n",
        "print(\"Problematic image paths:\", problematic_paths)\n"
      ],
      "metadata": {
        "id": "gSrptRVt6CB_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.models import Sequential\n",
        "from keras.layers import Conv2D, MaxPooling2D, Flatten, Dense\n",
        "\n",
        "# Define the CNN model\n",
        "def build_cnn(input_shape, num_classes):\n",
        "    model = Sequential([\n",
        "        Conv2D(32, (3, 3), activation='sigmoid', input_shape=input_shape),\n",
        "        MaxPooling2D((2, 2)),\n",
        "        Conv2D(64, (3, 3), activation='sigmoid'),\n",
        "        MaxPooling2D((2, 2)),\n",
        "        Conv2D(128, (3, 3), activation='sigmoid'),\n",
        "        MaxPooling2D((2, 2)),\n",
        "        Flatten(),\n",
        "        Dense(128, activation='sigmoid'),\n",
        "        Dense(num_classes, activation='softmax')\n",
        "    ])\n",
        "    return model\n",
        "\n",
        "# Set input shape\n",
        "input_shape = (28, 28, 1)  # Assuming images are grayscale\n",
        "\n",
        "# Number of classes\n",
        "num_classes = len(np.unique(y_shape_train))\n",
        "\n",
        "# Build the CNN model\n",
        "cnn_model_shape = build_cnn(input_shape, num_classes)\n",
        "\n",
        "\n",
        "# Print model summary\n",
        "cnn_model_shape.summary()\n"
      ],
      "metadata": {
        "id": "OQF_dpCuixOh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Compile the model\n",
        "# cnn_model_shape.compile(loss='sparse_categorical_crossentropy',\n",
        "#                   optimizer='SGD',\n",
        "#                   metrics=['accuracy'])\n",
        "\n",
        "# Compile the model\n",
        "cnn_model_shape.compile(optimizer='adam',\n",
        "              loss='sparse_categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "# Train the model\n",
        "# history_cnn_shape = cnn_model_shape.fit(X_train_shape_cnn, y_shape_train, epochs=10, batch_size=32, verbose=1)\n",
        "# Fit the CNN model on the training and validation data\n",
        "history_cnn_shape = cnn_model_shape.fit(X_train_shape_cnn, y_shape_train, epochs=10, batch_size=32, validation_data=(X_val_shape_cnn, y_shape_val))\n"
      ],
      "metadata": {
        "id": "x0QpM9Mbwc9C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate the model on the training data\n",
        "train_loss_cnn_shape, train_accuracy_cnn_shape = cnn_model_shape.evaluate(X_train_shape_cnn, y_shape_train)\n",
        "\n",
        "# Print the training accuracy and loss\n",
        "print(\"Training Accuracy (CNN) for Shapes:\", train_accuracy_cnn_shape)\n",
        "print(\"Training Loss (CNN) for Shapes:\", train_loss_cnn_shape)\n",
        "\n",
        "# Evaluate the model on the validation data\n",
        "val_loss_cnn_shape, val_accuracy_cnn_shape = cnn_model_shape.evaluate(X_val_shape_cnn, y_shape_val)\n",
        "\n",
        "# Print the validation accuracy and loss\n",
        "print(\"Validation Accuracy (CNN) for Shapes:\", val_accuracy_cnn_shape)\n",
        "print(\"Validation Loss (CNN) for Shapes:\", val_loss_cnn_shape)\n",
        "\n",
        "# Evaluate the model on the test data\n",
        "test_loss_cnn_shape, test_accuracy_cnn_shape = cnn_model_shape.evaluate(X_test_shape_cnn, y_shape_test)\n",
        "\n",
        "# Print the test accuracy and loss\n",
        "print(\"Test Accuracy (CNN) for Shapes:\", test_accuracy_cnn_shape)\n",
        "print(\"Test Loss (CNN) for Shapes:\", test_loss_cnn_shape)\n"
      ],
      "metadata": {
        "id": "Rm7wO_Na8HQM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot learning curves using the custom function\n",
        "plot_learning_curve(history_cnn_shape.history['loss'], history_cnn_shape.history['val_loss'],\n",
        "                    history_cnn_shape.history['accuracy'], history_cnn_shape.history['val_accuracy'],\n",
        "                    metric_name='Accuracy')\n"
      ],
      "metadata": {
        "id": "gjIxR7vb9X8W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Shape of X_train_shape_cnn:\", X_train_shape_cnn.shape)\n",
        "print(\"Shape of y_shape_train:\", y_shape_train.shape)\n"
      ],
      "metadata": {
        "id": "7s9rqd6tCbdJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_shape_test.shape"
      ],
      "metadata": {
        "id": "CSDfpzrb4t5O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_shape_test.shape"
      ],
      "metadata": {
        "id": "DC_H2fEm4wi8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **2.2 Tuning Parameters use GridSearchCV**"
      ],
      "metadata": {
        "id": "31ItEu9GIK2Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.wrappers.scikit_learn import KerasClassifier\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "# Define the function to create the CNN model\n",
        "def create_cnn_model(optimizer='adam', activation='relu'):\n",
        "    model = Sequential([\n",
        "        Conv2D(32, (3, 3), activation=activation, input_shape=(28, 28, 1)),\n",
        "        MaxPooling2D((2, 2)),\n",
        "        Conv2D(64, (3, 3), activation=activation),\n",
        "        MaxPooling2D((2, 2)),\n",
        "        Conv2D(128, (3, 3), activation=activation),\n",
        "        MaxPooling2D((2, 2)),\n",
        "        Flatten(),\n",
        "        Dense(128, activation=activation),\n",
        "        Dense(num_classes, activation='softmax')\n",
        "    ])\n",
        "    model_cnn_tune.compile(loss='sparse_categorical_crossentropy',\n",
        "                  optimizer=optimizer,\n",
        "                  metrics=['accuracy'])\n",
        "    return model_cnn_tune\n",
        "\n",
        "# Create a KerasClassifier based on the CNN model function\n",
        "cnn_shape_tune = KerasClassifier(build_fn=create_cnn_model, epochs=10, batch_size=32, verbose=0)\n",
        "\n",
        "# Define the hyperparameters to search\n",
        "param_grid = {\n",
        "    'optimizer': ['adam', 'sgd'],\n",
        "    'activation': ['relu', 'sigmoid']\n",
        "}\n",
        "\n",
        "# Perform grid search\n",
        "grid = GridSearchCV(estimator=cnn_shape_tune, param_grid=param_grid, cv=3)\n",
        "grid_result = grid.fit(X_train_shape_cnn, y_shape_train)\n",
        "\n",
        "# Print the best parameters found\n",
        "print(\"Best parameters: \", grid_result.best_params_)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 393
        },
        "id": "5rxdF_pIJuiP",
        "outputId": "d28a4e22-37ef-4d96-9b45-ecb5a4f74c44"
      },
      "execution_count": 99,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'keras.wrappers'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-99-521bf96aa0f1>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrappers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscikit_learn\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mKerasClassifier\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_selection\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mGridSearchCV\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# Define the function to create the CNN model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mcreate_cnn_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'adam'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactivation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'relu'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'keras.wrappers'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip uninstall tensorflow\n",
        "!pip install tensorflow"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "PTr2_9X4MrKT",
        "outputId": "3d241431-c679-49ce-faa3-e1edc76bae9f"
      },
      "execution_count": 100,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found existing installation: tensorflow 2.15.0\n",
            "Uninstalling tensorflow-2.15.0:\n",
            "  Would remove:\n",
            "    /usr/local/bin/estimator_ckpt_converter\n",
            "    /usr/local/bin/import_pb_to_tensorboard\n",
            "    /usr/local/bin/saved_model_cli\n",
            "    /usr/local/bin/tensorboard\n",
            "    /usr/local/bin/tf_upgrade_v2\n",
            "    /usr/local/bin/tflite_convert\n",
            "    /usr/local/bin/toco\n",
            "    /usr/local/bin/toco_from_protos\n",
            "    /usr/local/lib/python3.10/dist-packages/tensorflow-2.15.0.dist-info/*\n",
            "    /usr/local/lib/python3.10/dist-packages/tensorflow/*\n",
            "Proceed (Y/n)? Y\n",
            "  Successfully uninstalled tensorflow-2.15.0\n",
            "Collecting tensorflow\n",
            "  Downloading tensorflow-2.16.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (589.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m589.8/589.8 MB\u001b[0m \u001b[31m779.2 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=23.5.26 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (24.3.25)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.5.4)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.2.0)\n",
            "Collecting h5py>=3.10.0 (from tensorflow)\n",
            "  Downloading h5py-3.11.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (5.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.3/5.3 MB\u001b[0m \u001b[31m82.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (18.1.1)\n",
            "Collecting ml-dtypes~=0.3.1 (from tensorflow)\n",
            "  Downloading ml_dtypes-0.3.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.2/2.2 MB\u001b[0m \u001b[31m70.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.3.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorflow) (24.0)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.20.3)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.31.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from tensorflow) (67.7.2)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.16.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.4.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (4.11.0)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.14.1)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.63.0)\n",
            "Collecting tensorboard<2.17,>=2.16 (from tensorflow)\n",
            "  Downloading tensorboard-2.16.2-py3-none-any.whl (5.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.5/5.5 MB\u001b[0m \u001b[31m32.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting keras>=3.0.0 (from tensorflow)\n",
            "  Downloading keras-3.3.3-py3-none-any.whl (1.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m64.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.37.0)\n",
            "Requirement already satisfied: numpy<2.0.0,>=1.23.5 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.25.2)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from astunparse>=1.6.0->tensorflow) (0.43.0)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.10/dist-packages (from keras>=3.0.0->tensorflow) (13.7.1)\n",
            "Collecting namex (from keras>=3.0.0->tensorflow)\n",
            "  Downloading namex-0.0.8-py3-none-any.whl (5.8 kB)\n",
            "Collecting optree (from keras>=3.0.0->tensorflow)\n",
            "  Downloading optree-0.11.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (311 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m311.2/311.2 kB\u001b[0m \u001b[31m30.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow) (2024.2.2)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.17,>=2.16->tensorflow) (3.6)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.17,>=2.16->tensorflow) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.17,>=2.16->tensorflow) (3.0.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard<2.17,>=2.16->tensorflow) (2.1.5)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich->keras>=3.0.0->tensorflow) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich->keras>=3.0.0->tensorflow) (2.16.1)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich->keras>=3.0.0->tensorflow) (0.1.2)\n",
            "Installing collected packages: namex, optree, ml-dtypes, h5py, tensorboard, keras, tensorflow\n",
            "  Attempting uninstall: ml-dtypes\n",
            "    Found existing installation: ml-dtypes 0.2.0\n",
            "    Uninstalling ml-dtypes-0.2.0:\n",
            "      Successfully uninstalled ml-dtypes-0.2.0\n",
            "  Attempting uninstall: h5py\n",
            "    Found existing installation: h5py 3.9.0\n",
            "    Uninstalling h5py-3.9.0:\n",
            "      Successfully uninstalled h5py-3.9.0\n",
            "  Attempting uninstall: tensorboard\n",
            "    Found existing installation: tensorboard 2.15.2\n",
            "    Uninstalling tensorboard-2.15.2:\n",
            "      Successfully uninstalled tensorboard-2.15.2\n",
            "  Attempting uninstall: keras\n",
            "    Found existing installation: keras 2.15.0\n",
            "    Uninstalling keras-2.15.0:\n",
            "      Successfully uninstalled keras-2.15.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "tf-keras 2.15.1 requires tensorflow<2.16,>=2.15, but you have tensorflow 2.16.1 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed h5py-3.11.0 keras-3.3.3 ml-dtypes-0.3.2 namex-0.0.8 optree-0.11.0 tensorboard-2.16.2 tensorflow-2.16.1\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "h5py",
                  "keras",
                  "ml_dtypes",
                  "tensorboard",
                  "tensorflow"
                ]
              },
              "id": "2e9eb40977a24996a34606a7a5f09c95"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8SWlzp3M_2Xy"
      },
      "source": [
        "## Data Augmentation\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "import numpy as np\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "# Initialize ImageDataGenerator for augmentation\n",
        "datagen = ImageDataGenerator(\n",
        "    rotation_range=20,  # Random rotation up to 20 degrees\n",
        "    width_shift_range=0.1,  # Randomly shift images horizontally (10% of total width)\n",
        "    height_shift_range=0.1,  # Randomly shift images vertically (10% of total height)\n",
        "    horizontal_flip=True,  # Randomly flip images horizontally\n",
        "    vertical_flip=True,  # Randomly flip images vertically\n",
        "    zoom_range=0.1,  # Randomly zoom images (by up to 10%)\n",
        "    fill_mode='nearest',  # Fill in missing pixels with nearest neighbor\n",
        "    rescale=1.0/255,  # Rescale pixel values to range [0, 1]\n",
        "    data_format='channels_last'  # Image data format: channels_last\n",
        ")\n",
        "\n",
        "# Combined lists for original and augmented data\n",
        "combined_X_shape_images = []\n",
        "combined_y_shape_labels = []\n",
        "\n",
        "# Apply augmentation to the entire training data for shapes\n",
        "total_augmented_shape_images = 0\n",
        "augmentation_limit = 2  # Set a limit for the number of augmented images per original image\n",
        "\n",
        "for img_path, label in zip(X_shape_train, y_shape_train):\n",
        "    img = cv2.imread(img_path, cv2.IMREAD_GRAYSCALE)  # Load the image as grayscale using OpenCV\n",
        "    img = np.expand_dims(img, axis=-1)  # Add channel dimension\n",
        "    img = np.expand_dims(img, axis=0)  # Add batch size dimension\n",
        "    augmented_images_ind_shape = []\n",
        "    num_augmented_images = 0\n",
        "    for augmented_img in datagen.flow(img, batch_size=1):\n",
        "        augmented_images_ind_shape.append(augmented_img[0])\n",
        "        num_augmented_images += 1\n",
        "        if num_augmented_images >= augmentation_limit:\n",
        "            break\n",
        "    # Append the original image and its augmented versions to the combined lists\n",
        "    combined_X_shape_images.append(img[0])  # Remove the extra dimension\n",
        "    combined_y_shape_labels.append(label)\n",
        "    combined_X_shape_images.extend(augmented_images_ind_shape)\n",
        "    combined_y_shape_labels.extend([label] * len(augmented_images_ind_shape))\n",
        "    total_augmented_shape_images += len(augmented_images_ind_shape)\n",
        "\n",
        "# Check the shape and size of the combined images\n",
        "print(\"Combined images shape:\", combined_X_shape_images[0].shape)  # Shape of the first image\n",
        "print(\"Number of combined images:\", len(combined_X_shape_images))\n",
        "\n",
        "# Check the size of the combined labels\n",
        "print(\"Number of combined labels:\", len(combined_y_shape_labels))\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7N6zRP_zWvCx",
        "outputId": "ca54d10f-34a6-4d99-d3d2-1dd4e9793248"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Combined images shape: (28, 28, 1)\n",
            "Number of combined images: 6657\n",
            "Number of combined labels: 6657\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "shape distribution plot for data augmentation"
      ],
      "metadata": {
        "id": "e5BcpoDG0pHH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from collections import defaultdict\n",
        "\n",
        "# Extract unique shape types\n",
        "shape_types = defaultdict(int)\n",
        "for shape in shapes:\n",
        "    shape_type = shape[:-1]  # Exclude the last dimension (channels)\n",
        "    shape_types[shape_type] += 1\n",
        "\n",
        "# Sort the shape types by counts\n",
        "sorted_shape_types = sorted(shape_types.items(), key=lambda x: x[1], reverse=True)\n",
        "\n",
        "# Extract the sorted shape types and counts\n",
        "sorted_shapes = [str(shape_type) for shape_type, count in sorted_shape_types]\n",
        "sorted_counts = [count for shape_type, count in sorted_shape_types]\n",
        "\n",
        "# Plot the distribution\n",
        "plt.figure(figsize=(12, 6))\n",
        "plt.bar(range(len(sorted_shapes)), sorted_counts, color='skyblue')\n",
        "plt.title('Distribution of Combined Image Shape Types by Count')\n",
        "plt.xlabel('Image Shape Type')\n",
        "plt.ylabel('Count')\n",
        "plt.xticks(range(len(sorted_shapes)), sorted_shapes, rotation=45, ha='right')\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 216
        },
        "id": "inDmYi7mwVZh",
        "outputId": "ecb2b4be-7c9f-41e5-d75b-5f5ce69ab154"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'shapes' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-38-c1ab8a4d20e9>\u001b[0m in \u001b[0;36m<cell line: 6>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# Extract unique shape types\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mshape_types\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdefaultdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mshape\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mshapes\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m     \u001b[0mshape_type\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m  \u001b[0;31m# Exclude the last dimension (channels)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0mshape_types\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mshape_type\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'shapes' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "type distribution plot for data augmentation"
      ],
      "metadata": {
        "id": "bmPUwngK0ytP"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "O4EEAwaJ11k9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "OilIEZeX0x7X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.models import Sequential\n",
        "from keras.layers import Conv2D, MaxPooling2D, Flatten, Dense\n",
        "\n",
        "# Define the CNN model with data augmentation\n",
        "def build_cnn_augmented(input_shape, num_classes):\n",
        "    model = Sequential([\n",
        "        Conv2D(32, (3, 3), activation='sigmoid', input_shape=input_shape),\n",
        "        MaxPooling2D((2, 2)),\n",
        "        Conv2D(64, (3, 3), activation='sigmoid'),\n",
        "        MaxPooling2D((2, 2)),\n",
        "        Conv2D(128, (3, 3), activation='sigmoid'),\n",
        "        MaxPooling2D((2, 2)),\n",
        "        Flatten(),\n",
        "        Dense(128, activation='sigmoid'),\n",
        "        Dense(num_classes, activation='softmax')\n",
        "    ])\n",
        "    return model\n",
        "\n",
        "# Set input shape\n",
        "input_shape = (28, 28, 1)  # Assuming images are grayscale\n",
        "\n",
        "# Number of classes\n",
        "num_classes = len(np.unique(y_shape_train))\n",
        "\n",
        "# Build the CNN model with data augmentation\n",
        "cnn_model_augmented = build_cnn_augmented(input_shape, num_classes)\n",
        "\n",
        "# Compile the model\n",
        "cnn_model_augmented.compile(loss='sparse_categorical_crossentropy',\n",
        "                            optimizer='SGD',\n",
        "                            metrics=['accuracy'])\n",
        "\n",
        "# Print model summary\n",
        "cnn_model_augmented.summary()\n",
        "\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "# Convert the input data to NumPy arrays\n",
        "combined_X_shape_images = np.array(combined_X_shape_images)\n",
        "combined_y_shape_labels = np.array(combined_y_shape_labels)\n",
        "\n",
        "# Train the CNN model with augmented data\n",
        "history_augmented = cnn_model_augmented.fit(combined_X_shape_images, combined_y_shape_labels, epochs=10, batch_size=32, verbose=1)\n"
      ],
      "metadata": {
        "id": "L_eC62busGH6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate the model on the test data\n",
        "test_loss, test_accuracy = cnn_model.evaluate(combined_X_shape_images, combined_y_shape_labels, verbose=1)\n",
        "\n",
        "# Print the test accuracy\n",
        "print(\"Test Accuracy:\", test_accuracy)\n"
      ],
      "metadata": {
        "id": "nSO-syFsbrk-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#  edge extraction on both train and test images (include augmented images)"
      ],
      "metadata": {
        "id": "jRgBozsaabvE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Edge Extraction\n",
        "\n",
        "Edge extraction can be a useful feature extraction technique for image classification tasks, as it helps capture important structural information present in the images."
      ],
      "metadata": {
        "id": "vvHIath--6iY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "combined_X_shape_images[0].shape\n"
      ],
      "metadata": {
        "id": "PXph4IwZ8L07"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_shape_test.shape"
      ],
      "metadata": {
        "id": "fALjB11r5dH0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "import numpy as np\n",
        "\n",
        "# Function to perform edge extraction on a single image\n",
        "def edge_extraction(image):\n",
        "    # Check if the input image is a NumPy array\n",
        "    if not isinstance(image, np.ndarray):\n",
        "        raise ValueError(\"Input image must be a NumPy array\")\n",
        "\n",
        "    # Apply Gaussian blur to reduce noise\n",
        "    blurred = cv2.GaussianBlur(image, (3, 3), 0)\n",
        "\n",
        "    # Check if the blurred image is of the correct data type\n",
        "    if blurred.dtype != np.uint8:\n",
        "        blurred = blurred.astype(np.uint8)\n",
        "\n",
        "    # Perform Canny edge detection\n",
        "    edges = cv2.Canny(blurred, 30, 150)\n",
        "\n",
        "    return edges\n",
        "\n",
        "# Convert combined_X_shape_images to a NumPy array if it's not already\n",
        "combined_X_shape_images = np.array(combined_X_shape_images)\n",
        "\n",
        "# Load images from paths in X_shape_test and convert them to NumPy arrays\n",
        "X_shape_test_images = []\n",
        "for image_path in X_shape_test:\n",
        "    img = cv2.imread(image_path, cv2.IMREAD_GRAYSCALE)  # Load image in grayscale\n",
        "    if img is None:\n",
        "        print(f\"Failed to load image from path: {image_path}\")\n",
        "    else:\n",
        "        X_shape_test_images.append(img)\n",
        "\n",
        "# Apply edge extraction to the train images\n",
        "X_train_edges = [edge_extraction(image) for image in combined_X_shape_images]\n",
        "\n",
        "# Apply edge extraction to the test images\n",
        "X_test_edges = [edge_extraction(image) for image in X_shape_test_images]\n",
        "\n",
        "# Convert lists to NumPy arrays\n",
        "X_train_edges = np.array(X_train_edges)\n",
        "X_test_edges = np.array(X_test_edges)\n",
        "\n",
        "# Print the shapes of the edge-extracted images\n",
        "print(\"Shape of train images after edge extraction:\", X_train_edges.shape)\n",
        "print(\"Shape of test images after edge extraction:\", X_test_edges.shape)\n"
      ],
      "metadata": {
        "id": "k4-6aQLaCYdp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# import numpy as np\n",
        "# from keras.models import Sequential\n",
        "# from keras.layers import Conv2D, MaxPooling2D, Flatten, Dense\n",
        "\n",
        "# # Define the CNN model architecture\n",
        "# def build_cnn_model(input_shape, num_classes):\n",
        "#     model = Sequential([\n",
        "#         Conv2D(32, (3, 3), activation='relu', input_shape=input_shape),\n",
        "#         MaxPooling2D((2, 2)),\n",
        "#         Conv2D(64, (3, 3), activation='relu'),\n",
        "#         MaxPooling2D((2, 2)),\n",
        "#         Flatten(),\n",
        "#         Dense(128, activation='relu'),\n",
        "#         Dense(num_classes, activation='softmax')\n",
        "#     ])\n",
        "#     return model\n",
        "\n",
        "# # Reshape the input data to include the channel dimension\n",
        "# X_train_edges = np.expand_dims(X_train_edges, axis=-1)\n",
        "# X_test_edges = np.expand_dims(X_test_edges, axis=-1)\n",
        "\n",
        "# # Set input shape (shape of the input images)\n",
        "# input_shape = X_train_edges[0].shape\n",
        "\n",
        "# # Number of classes (assuming it's known)\n",
        "# num_classes = 10  # Adjust this according to your dataset\n",
        "\n",
        "# # Build the CNN model\n",
        "# cnn_model = build_cnn_model(input_shape, num_classes)\n",
        "\n",
        "# # Compile the model\n",
        "# cnn_model.compile(optimizer='SGD',\n",
        "#                   loss='sparse_categorical_crossentropy',\n",
        "#                   metrics=['accuracy'])\n",
        "\n",
        "# # Print model summary\n",
        "# cnn_model.summary()\n",
        "\n",
        "# # Train the CNN model\n",
        "# history = cnn_model.fit(X_train_edges, combined_y_shape_labels, epochs=10, validation_data=(X_test_edges, y_shape_test))\n"
      ],
      "metadata": {
        "id": "RudnCZhfC5vD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Conv2D, MaxPooling2D, Flatten, Dense\n",
        "\n",
        "# Define the CNN model architecture\n",
        "def build_cnn_model(input_shape, num_classes):\n",
        "    model = Sequential([\n",
        "        Conv2D(32, (3, 3), activation='sigmoid', input_shape=input_shape),\n",
        "        MaxPooling2D((2, 2)),\n",
        "        Conv2D(64, (3, 3), activation='sigmoid'),\n",
        "        MaxPooling2D((2, 2)),\n",
        "        Flatten(),\n",
        "        Dense(128, activation='sigmoid'),\n",
        "        Dense(num_classes, activation='softmax')\n",
        "    ])\n",
        "    return model\n",
        "\n",
        "# Reshape the input data to include the channel dimension\n",
        "X_train_edges = np.expand_dims(X_train_edges, axis=-1)\n",
        "X_test_edges = np.expand_dims(X_test_edges, axis=-1)\n",
        "\n",
        "# Set input shape (shape of the input images)\n",
        "input_shape = X_train_edges[0].shape\n",
        "\n",
        "# Number of classes (assuming it's known)\n",
        "num_classes = 10  # Adjust this according to your dataset\n",
        "\n",
        "# Build the CNN model\n",
        "cnn_model = build_cnn_model(input_shape, num_classes)\n",
        "\n",
        "# Compile the model\n",
        "cnn_model.compile(optimizer='SGD',\n",
        "                  loss='sparse_categorical_crossentropy',\n",
        "                  metrics=['accuracy'])\n",
        "\n",
        "# Print model summary\n",
        "cnn_model.summary()\n",
        "\n",
        "# Train the CNN model\n",
        "history = cnn_model.fit(X_train_edges, combined_y_shape_labels, epochs=10, validation_data=(X_test_edges, y_shape_test))\n"
      ],
      "metadata": {
        "id": "cH8timYxFLGp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Shape of X_train_edges:\", X_train_edges.shape)\n",
        "print(\"Shape of X_test_edges:\", X_test_edges.shape)\n"
      ],
      "metadata": {
        "id": "dehC50JTFQ_w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate the model on the test data\n",
        "loss, accuracy = cnn_model.evaluate(X_test_edges, y_shape_test)\n",
        "\n",
        "print(f'Test Loss: {loss:.4f}')\n",
        "print(f'Test Accuracy: {accuracy:.4f}')\n"
      ],
      "metadata": {
        "id": "l1SDmmK8Eq2x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train_edges.shape"
      ],
      "metadata": {
        "id": "AHSzuiCkC5xn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Hyperparameters Grid for Optimization"
      ],
      "metadata": {
        "id": "HCGlN-L4Ig4g"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# early stoping for overfitting - baseline\n",
        "regulization and drop out\n",
        "tuning lamba = 1,,2 ..\n",
        "increase regulization and drop out\n",
        "error analysis"
      ],
      "metadata": {
        "id": "u8S36b7v9FdL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense\n",
        "from tensorflow.keras.wrappers.scikit_learn import KerasClassifier\n",
        "\n",
        "# Define a function to build the CNN model\n",
        "def build_cnn_model(num_filters=32, kernel_size=(3, 3), pool_size=(2, 2), num_neurons=128, activation='relu'):\n",
        "    model = Sequential([\n",
        "        Conv2D(num_filters, kernel_size, activation=activation, input_shape=(28, 28, 1)),\n",
        "        MaxPooling2D(pool_size),\n",
        "        Flatten(),\n",
        "        Dense(num_neurons, activation=activation),\n",
        "        Dense(10, activation='softmax')\n",
        "    ])\n",
        "    modelhttps://colab.research.google.com/drive/1p3tPtwnc4SNITXCYGuXVW4ujqFTtZLjO#scrollTo=early_stoping_for_overfitting_baseline.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "    return model\n",
        "\n",
        "# Ensure the shapes of features and labels are consistent\n",
        "print(\"Shape of X_train_edges:\", X_train_edges.shape)\n",
        "print(\"Shape of y_shape_train:\", y_shape_train.shape)\n",
        "\n",
        "# Define the hyperparameters grid\n",
        "param_grid = {\n",
        "    'num_filters': [32, 64],\n",
        "    'kernel_size': [(3, 3), (5, 5)],\n",
        "    'pool_size': [(2, 2), (3, 3)],\n",
        "    'num_neurons': [128, 256],\n",
        "    'activation': ['relu', 'sigmoid']\n",
        "}\n",
        "\n",
        "# Create a KerasClassifier with the build_cnn_model function\n",
        "keras_classifier = KerasClassifier(build_cnn_model, epochs=5, batch_size=32, verbose=0)\n",
        "\n",
        "# Perform grid search\n",
        "grid = GridSearchCV(estimator=keras_classifier, param_grid=param_grid, cv=3, verbose=1)\n",
        "grid_result = grid.fit(X_train_edges, y_shape_train)\n",
        "\n",
        "# Summarize results\n",
        "print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\n",
        "means = grid_result.cv_results_['mean_test_score']\n",
        "stds = grid_result.cv_results_['std_test_score']\n",
        "params = grid_result.cv_results_['params']\n",
        "for mean, stdev, param in zip(means, stds, params):\n",
        "    print(\"%f (%f) with: %r\" % (mean, stdev, param))\n"
      ],
      "metadata": {
        "id": "KLg8z0_MJtY2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# !pip install --upgrade tensorflow\n"
      ],
      "metadata": {
        "id": "1bvBq-dkKAlt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Regularization Techniques\n"
      ],
      "metadata": {
        "id": "nm_-pykRK7-4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Learning Rate Scheduling"
      ],
      "metadata": {
        "id": "j9ohcVG1LfSc"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KIYei9VUHwXx"
      },
      "source": [
        "In this report, a Convolutional Neural Network (CNN) is employed as the baseline model for image classification tasks owing to its aptitude in handling visual data. CNNs are adept at automatically extracting relevant features from images through layers like convolutional and pooling layers, enabling them to discern intricate patterns, textures, and shapes. Their hierarchical learning approach facilitates the capturing of complex relationships within images, making them ideal for tasks like image classification. Additionally, CNNs have exhibited exceptional performance across various computer vision applications and are particularly robust to variations in input data, further solidifying their suitability as a baseline model for image classification endeavors."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4LWsCIjrC3ZE"
      },
      "source": [
        "This function iterates over each row in the DataFrame, loads the corresponding image using OpenCV, resizes it to 28x28 pixels, normalizes the pixel values, and stores the preprocessed images and labels in lists. Finally, it converts the lists to numpy arrays and adds a channel dimension for compatibility with Conv2D layers in Keras."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BVmh1T0wEB4F"
      },
      "source": [
        "The fit_transform() method fits the encoder to the training labels and transforms them into encoded numerical labels. Then, the transform() method is used to encode the validation and test labels using the same encoder fitted on the training labels. This ensures consistency in label encoding across different datasets."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hb50FmTbESqU"
      },
      "source": [
        "- `train_images` are the preprocessed images for training.\n",
        "- `train_labels_encoded` are the encoded numerical labels corresponding to the training images.\n",
        "- `epochs=10` specifies the number of training epochs.\n",
        "- `validation_data=(val_images, val_labels_encoded)` specifies the validation data to evaluate the model's performance during training. `val_images` are the preprocessed images for validation, and `val_labels_encoded` are the encoded numerical labels corresponding to the validation images."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jrHehM1rDN3P"
      },
      "source": [
        "This code defines a convolutional neural network (CNN) model using the Sequential API. It consists of three convolutional layers followed by max-pooling layers to downsample the feature maps. The final convolutional layer is followed by a flatten layer to transition from convolutional to dense layers. Two dense layers are added for classification, with the output layer having softmax activation to output class probabilities. The model is compiled with the Adam optimizer, sparse categorical crossentropy loss function, and accuracy as the evaluation metric."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SwXa8z0POS-F"
      },
      "source": [
        "Enhancing the baseline CNN model by Dropout Regularization."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SAldCpI7N-xE"
      },
      "source": [
        "Explination:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tVBukSFCOPw2"
      },
      "source": [
        "Enhancing the baseline CNN model by incorporating techniques such as learning rate scheduling and data augmentation."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C4rPI4wIObYO"
      },
      "source": [
        "Explination:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HHvUQFgJ8ndP"
      },
      "outputs": [],
      "source": [
        "# Reshape the images to 2D arrays\n",
        "train_images_2d = train_images.reshape(train_images.shape[0], -1)\n",
        "test_images_2d = test_images.reshape(test_images.shape[0], -1)\n",
        "\n",
        "# Define the pipeline with preprocessing steps and the model\n",
        "pipeline = Pipeline([\n",
        "    ('scaler', StandardScaler()),  # Add preprocessing steps if necessary\n",
        "    ('model', RandomForestClassifier())  # RandomForestClassifier with your desired model\n",
        "])\n",
        "\n",
        "# Define hyperparameters grid for optimization\n",
        "param_grid = {\n",
        "    'model__n_estimators': [100, 200, 300],\n",
        "    'model__max_depth': [None, 10, 20],\n",
        "    # Add more hyperparameters to optimize as needed\n",
        "}\n",
        "\n",
        "# Perform grid search cross-validation to find the best hyperparameters\n",
        "grid_search = GridSearchCV(pipeline, param_grid, cv=5, verbose=1, n_jobs=-1)\n",
        "grid_search.fit(train_images_2d, train_labels_encoded)\n",
        "\n",
        "# Get the best model and evaluate its performance\n",
        "best_model = grid_search.best_estimator_\n",
        "test_accuracy = best_model.score(test_images_2d, test_labels_encoded)\n",
        "print(\"Optimized Model Test Accuracy:\", test_accuracy)\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}